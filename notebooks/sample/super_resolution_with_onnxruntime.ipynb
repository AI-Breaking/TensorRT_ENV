{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.16.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.18.1-cp38-cp38-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from onnx) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx) (4.25.3)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (24.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnx-1.16.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.18.1-cp38-cp38-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.16.1 onnxruntime-1.18.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Google Colab에서 노트북을 실행하실 때에는 \n",
    "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
    "%matplotlib inline\n",
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기\n",
    "================================================================\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>PyTorch 2.1부터 ONNX Exporter에는 두 가지 버전이 존재합니다.<em> <code>torch.onnx.dynamo_export</code> 는 PyTorch 2.0과 함께 출시된 TorchDynamo 기술 기반의 최신(이지만 아직 베타 버전의) ONNX Exporter입니다.</em> <code>torch.onnx.export</code> 는 PyuTorch 1.2.0부터 지원 중인 TorchScript 백엔드에 기반한 ONNX Exporter입니다.</p>\n",
    "</div>\n",
    "\n",
    "이 튜토리얼에서는 TorchScript 기반의 ONNX Exporter인 `torch.onnx.export`\n",
    "를 사용하여 PyTorch에서 정의한 모델을 어떻게 ONNX 형식으로 변환하는지를\n",
    "살펴보도록 하겠습니다.\n",
    "\n",
    "이렇게 변환된 모델은 ONNX 런타임(Runtime)에서 실행됩니다. ONNX 런타임은\n",
    "다양한 플랫폼과 하드웨어(윈도우즈, 리눅스, 맥 및 CPU, GPU 모두)에서\n",
    "효율적으로 추론하는, 성능에 초점을 맞춘 ONNX 모델을 위한 엔진입니다.\n",
    "\n",
    "[여기](https://cloudblogs.microsoft.com/opensource/2019/05/22/onnx-runtime-machine-learning-inferencing-0-4-release)\n",
    "에서 설명한 것처럼 ONNX 런타임을 활용하면 여러 모델들의 성능을 상당히\n",
    "높일 수 있다는 것이 증명되었습니다.\n",
    "\n",
    "이 튜토리얼의 진행을 위해 [ONNX](https://github.com/onnx/onnx) 및 [ONNX\n",
    "런타임(Runtime)](https://github.com/microsoft/onnxruntime) 의 설치가\n",
    "필요합니다.\n",
    "\n",
    "ONNX 및 ONNX 런타임은 다음과 같이 설치할 수 있습니다:\n",
    "\n",
    "``` {.sourceCode .bash}\n",
    "%%bash\n",
    "pip install onnx onnxruntime\n",
    "```\n",
    "\n",
    "ONNX 런타임은 최신 버전의 PyTorch 런타임을 사용하는 것을 권장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 import문\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초해상화(super-resolution)란 이미지나 비디오의 해상도를 높이기 위한\n",
    "방법으로 이미지 프로세싱이나 비디오 편집에 널리 사용되고 있는\n",
    "방법입니다. 이 튜토리얼에서는 크기가 작은 초해상화 모델을 사용하도록\n",
    "하겠습니다.\n",
    "\n",
    "먼저, 초해상화 모델을 PyTorch에서 구현하겠습니다. 이 모델은 [\\\"Real-Time\n",
    "Single Image and Video Super-Resolution Using an Efficient Sub-Pixel\n",
    "Convolutional Neural Network\\\" - Shi et\n",
    "al](https://arxiv.org/abs/1609.05158) 에서 소개된 효율적인 서브픽셀\n",
    "합성곱 계층을 사용하여 이미지의 해상도를 업스케일 인자만큼 늘립니다.\n",
    "모델은 이미지의 YCbCr 성분 중 Y 성분을 입력값으로 받고 업스케일된\n",
    "초해상도의 Y 채널 값을 리턴합니다.\n",
    "\n",
    "아래는 PyTorch 예제의\n",
    "[모델](https://github.com/pytorch/examples/blob/master/super_resolution/model.py)\n",
    "을 그대로 가져온 것입니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch에서 구현된 초해상도 모델\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "\n",
    "# 위에서 정의된 모델을 사용하여 초해상도 모델 생성\n",
    "torch_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 과정이 끝나면 일반적인 경우에 모델을 학습시키기 시작할 것입니다.\n",
    "하지만 본 튜토리얼에서는 미리 학습된 가중치들을 사용하도록 하겠습니다.\n",
    "참고로 이 모델은 높은 정확도에 이를 때까지 학습되지 않았고 본 튜토리얼을\n",
    "원활히 진행하기 위한 목적으로 사용하는 것입니다.\n",
    "\n",
    "모델을 변환하기 전에 모델을 추론 모드로 바꾸기 위해서\n",
    "`torch_model.eval()` 또는 `torch_model.train(False)` 를 호출하는 것이\n",
    "중요합니다. 이는 dropout이나 batchnorm과 같은 연산들이 추론과 학습\n",
    "모드에서 다르게 작동하기 때문에 필요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperResolutionNet(\n",
       "  (relu): ReLU()\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미리 학습된 가중치를 읽어옵니다\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1    # 임의의 수\n",
    "\n",
    "# 모델을 미리 학습된 가중치로 초기화합니다\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# 모델을 추론 모드로 전환합니다\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Tracing이나 스크립팅을 통해서 PyTorch 모델을 변환할 수 있습니다. 이\n",
    "튜토리얼에서는 tracing을 통해 변환된 모델을 사용하도록 하겠습니다.\n",
    "모델을 변환하기 위해서는 `torch.onnx.export()` 함수를 호출합니다. 이\n",
    "함수는 모델을 실행하여 어떤 연산자들이 출력값을 계산하는데\n",
    "사용되었는지를 기록합니다. `export` 함수가 모델을 실행하기 때문에,\n",
    "우리가 직접 텐서를 입력값으로 넘겨주어야 합니다. 이 텐서의 값은 알맞은\n",
    "자료형과 모양이라면 랜덤하게 결정되어도 무방합니다. 특정 차원을 동적\n",
    "차원으로 지정하지 않는 이상, ONNX로 변환된 그래프의 경우 입력값의 크기는\n",
    "모든 차원에 대해 고정됩니다. 예시에서는 모델이 항상 배치 사이즈 1을\n",
    "사용하도록 변환하였지만, `torch.onnx.export()` 의 `dynamic_axes` 인자의\n",
    "첫번째 차원은 동적 차원으로 지정합니다. 따라서 변환된 모델은 임의의\n",
    "batch\\_size에 대해 \\[batch\\_size, 1, 224, 224\\] 사이즈 입력값을 받을 수\n",
    "있습니다.\n",
    "\n",
    "PyTorch의 변환 인터페이스에 대해 더 자세히 알고 싶다면 [torch.onnx\n",
    "문서](https://pytorch.org/docs/master/onnx.html) 를 참고해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 모델에 대한 입력값\n",
    "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_out = torch_model(x)\n",
    "\n",
    "# 모델 변환\n",
    "torch.onnx.export(torch_model,               # 실행될 모델\n",
    "                  x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)\n",
    "                  \"super_resolution.onnx\",   # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)\n",
    "                  export_params=True,        # 모델 파일 안에 학습된 모델 가중치를 저장할지의 여부\n",
    "                  opset_version=10,          # 모델을 변환할 때 사용할 ONNX 버전\n",
    "                  do_constant_folding=True,  # 최적화시 상수폴딩을 사용할지의 여부\n",
    "                  input_names = ['input'],   # 모델의 입력값을 가리키는 이름\n",
    "                  output_names = ['output'], # 모델의 출력값을 가리키는 이름\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # 가변적인 길이를 가진 차원\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX 런타임에서 변환된 모델을 사용했을 때 같은 결과를 얻는지 확인하기\n",
    "위해서 `torch_out` 를 계산합니다.\n",
    "\n",
    "ONNX 런타임에서의 모델 결과값을 확인하기 전에 먼저 ONNX API를 사용해\n",
    "ONNX 모델을 확인해보도록 하겠습니다. 먼저,\n",
    "`onnx.load(\"super_resolution.onnx\")` 는 저장된 모델을 읽어온 후 머신러닝\n",
    "모델을 취합하여 저장하고 있는 상위 파일 컨테이너인 `onnx.ModelProto` 를\n",
    "리턴합니다. `onnx.ModelProto` 에 대해 더 자세한 것은 [onnx.proto\n",
    "기술문서](https://github.com/onnx/onnx/blob/master/onnx/onnx.proto) 에서\n",
    "확인하실 수 있습니다. `onnx.checker.check_model(onnx_model)` 는 모델의\n",
    "구조를 확인하고 모델이 유효한 스키마(valid schema)를 가지고 있는지를\n",
    "체크합니다. ONNX 그래프의 유효성은 모델의 버전, 그래프 구조, 노드들,\n",
    "그리고 입력값과 출력값들을 모두 체크하여 결정됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"super_resolution.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ONNX 런타임의 Python API를 통해 결과값을 계산해보도록 하겠습니다.\n",
    "이 부분은 보통 별도의 프로세스 또는 별도의 머신에서 실행되지만, 이\n",
    "튜토리얼에서는 모델이 ONNX 런타임과 PyTorch에서 동일한 결과를\n",
    "출력하는지를 확인하기 위해 동일한 프로세스에서 계속 실행하도록\n",
    "하겠습니다.\n",
    "\n",
    "모델을 ONNX 런타임에서 실행하기 위해서는 미리 설정된 인자들(본\n",
    "예제에서는 기본값을 사용합니다)로 모델을 위한 추론 세션을 생성해야\n",
    "합니다. 세션이 생성되면, 모델의 run() API를 사용하여 모델을 실행합니다.\n",
    "이 API의 리턴값은 ONNX 런타임에서 연산된 모델의 결과값들을 포함하고 있는\n",
    "리스트입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# ONNX 런타임에서 계산된 결과값\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# ONNX 런타임과 PyTorch에서 연산된 결과값 비교\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 PyTorch와 ONNX 런타임에서 연산된 결과값이 서로 일치하는지 오차범위(\n",
    "`rtol=1e-03` 와 `atol=1e-05`) 이내에서 확인해야 합니다. 만약 결과가\n",
    "일치하지 않는다면 ONNX 변환기에 문제가 있는 것이니 저희에게 알려주시기\n",
    "바랍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX 런타임에서 이미지를 입력값으로 모델을 실행하기\n",
    "===================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 PyTorch 모델을 변환하고 어떻게 ONNX 런타임에서 구동하는지\n",
    "가상의 텐서를 입력값으로 하여 살펴보았습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 튜토리얼에서는 아래와 같은 유명한 고양이 사진을 사용하도록\n",
    "하겠습니다.\n",
    "\n",
    "![](https://tutorials.pytorch.kr/_static/img/cat_224x224.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, PIL 라이브러리를 사용하여 이미지를 로드하고 전처리하겠습니다. 이\n",
    "전처리는 신경망 학습과 테스트에 보편적으로 적용되고 있는 전처리\n",
    "과정입니다.\n",
    "\n",
    "먼저 이미지를 모델의 입력값 크기(224x224)에 맞게 리사이즈합니다.\n",
    "\n",
    ":   그리고 이미지를 Y, Cb, Cr 성분으로 분해합니다.\n",
    "\n",
    "Y 성분\\[역자 주: 휘도 성분\\]은 그레이스케일(회색조) 이미지를 나타내고,\n",
    "Cb 성분은 파란색에서 밝기를 뺀 색차 성분, Cr은 빨강색에서 밝기를 뺀 색차\n",
    "성분을 나타냅니다. 사람의 눈은 Y 성분에 더 민감하게 반응하기 때문에\n",
    "저희에게는 현재 이 성분이 중요하고, 이 Y 성분을 변환할 것입니다. Y\n",
    "성분을 뽑아낸 뒤에, 추출한 Y 성분을 모델의 입력값이 될 텐서로\n",
    "변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2157, 0.1961, 0.1922,  ..., 0.5294, 0.5569, 0.5686],\n",
       "          [0.2039, 0.1961, 0.1922,  ..., 0.5333, 0.5569, 0.5686],\n",
       "          [0.1961, 0.1843, 0.1843,  ..., 0.5216, 0.5412, 0.5490],\n",
       "          ...,\n",
       "          [0.6667, 0.6745, 0.6392,  ..., 0.6902, 0.6667, 0.6078],\n",
       "          [0.6392, 0.6431, 0.6235,  ..., 0.8000, 0.7608, 0.6745],\n",
       "          [0.6392, 0.6353, 0.6510,  ..., 0.8118, 0.7686, 0.6667]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img = Image.open(\"./cat.jpeg\")\n",
    "\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "\n",
    "img_ycbcr = img.convert('YCbCr')\n",
    "img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_y = to_tensor(img_y)\n",
    "img_y.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 리사이즈된 그레이스케일 고양이 이미지 텐서를 앞서 설명했던 것처럼\n",
    "초해상도 모델에 넘겨주어 ONNX 런타임에서 실행하도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_y = ort_outs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 하면 모델의 결과값은 텐서가 됩니다. 이제 모델의 결과값을 처리하여\n",
    "결과값 텐서에서 마지막 최종 출력 이미지를 만들고 이를 저장해보도록\n",
    "하겠습니다. 후처리 단계는\n",
    "[링크](https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py)\n",
    "에 구현되어있는 초해상도 모델 코드에서 가져온 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n",
    "\n",
    "# PyTorch 버전의 후처리 과정 코드를 이용해 결과 이미지 만들기\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")\n",
    "\n",
    "# 이미지를 저장하고 모바일 기기에서의 결과 이미지와 비교하기\n",
    "final_img.save(\"./cat_superres_with_ort.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tutorials.pytorch.kr/_static/img/cat_superres_with_ort.jpg)\n",
    "\n",
    "ONNX 런타임은 크로스플랫폼 엔진으로서 CPU와 GPU 뿐만 아니라 여러\n",
    "플랫폼에서 실행될 수 있습니다.\n",
    "\n",
    "ONNX 런타임은 Azure Machine Learning Services와 같은 클라우드에 배포되어\n",
    "모델 추론을 하는데 사용될 수도 있습니다. 더 자세한 내용은\n",
    "[링크](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-onnx)\n",
    "를 참조해주십시오.\n",
    "\n",
    "ONNX 런타임의 성능에 관한 것은\n",
    "[여기](https://github.com/microsoft/onnxruntime#high-performance) 에서\n",
    "확인하실 수 있습니다.\n",
    "\n",
    "ONNX 런타임에 관한 더 자세한 내용은 [이\n",
    "곳](https://github.com/microsoft/onnxruntime) 을 참조해 주세요.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
