{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.8/dist-packages (1.16.1)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.8/dist-packages (1.18.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from onnx) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx) (4.25.3)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (24.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Google Colab에서 노트북을 실행하실 때에는 \n",
    "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
    "%matplotlib inline\n",
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "data_root = './ReDNetPlus/dataset/Harvey_harvey-feb13-2021-2'\n",
    "train_list = './ReDNetPlus/dataset/cityscapes/list/fine_train.txt'\n",
    "val_list = './ReDNetPlus/dataset/cityscapes/list/fine_val.txt'\n",
    "classes = 13\n",
    "\n",
    "arch = 'red'\n",
    "backbone = 'resnet101'\n",
    "layers = 101\n",
    "dataset_name = 'harvey'\n",
    "sync_bn = True\n",
    "train_h = 713\n",
    "train_w = 713\n",
    "scale_min = 0.5\n",
    "scale_max = 2.0\n",
    "rotate_min = -10\n",
    "rotate_max = 10\n",
    "zoom_factor = 8\n",
    "ignore_label = 255\n",
    "aux_weight = 0.4\n",
    "train_gpu = [0]\n",
    "workers = 4\n",
    "batch_size = 2\n",
    "batch_size_val = 1\n",
    "base_lr = 0.001\n",
    "epochs = 100\n",
    "start_epoch = 0\n",
    "power = 0.9\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00001\n",
    "print_freq = 10\n",
    "save_freq = 1\n",
    "save_path = './ReDNetPlus/exp/Harvey_harvey-feb13-2021-2/pspnet101/model'\n",
    "log_dir = './logs'  # 로그 디렉토리 설정\n",
    "weight = None\n",
    "resume = None\n",
    "evaluate = True\n",
    "\n",
    "dist_url = 'tcp://127.0.0.1:6789'\n",
    "dist_backend = 'nccl'\n",
    "multiprocessing_distributed = True\n",
    "world_size = 1\n",
    "rank = 0\n",
    "use_apex = True\n",
    "opt_level = 'O0'\n",
    "\n",
    "test_list = './ReDNetPlus/dataset/harvey/list/test_img_list.txt'\n",
    "split = 'val'\n",
    "mode = 'vis'\n",
    "base_size = 2048\n",
    "test_h = 713\n",
    "test_w = 713\n",
    "scales = [1.0]\n",
    "has_prediction = False\n",
    "index_start = 0\n",
    "index_step = 0\n",
    "test_gpu = [0]\n",
    "model_path = './ReDNetPlus/exp/Harvey_harvey-feb13-2021-2/pspnet101/model/train_epoch_100.pth'\n",
    "save_folder = './ReDNetPlus/exp/Harvey_harvey-feb13-2021-2/all-cls/713x713/Iter-100/exp-1/result'\n",
    "colors_path = './ReDNetPlus/dataset/michael/list/michael_colors.txt'\n",
    "names_path = './ReDNetPlus/dataset/michael/list/michael_names.txt'\n",
    "imshow_batch = True\n",
    "device = 'cuda'\n",
    "ignore_unlabeled = True\n",
    "print_step = False\n",
    "output = 'outputs'\n",
    "dataset_dir = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기\n",
    "================================================================\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>PyTorch 2.1부터 ONNX Exporter에는 두 가지 버전이 존재합니다.<em> <code>torch.onnx.dynamo_export</code> 는 PyTorch 2.0과 함께 출시된 TorchDynamo 기술 기반의 최신(이지만 아직 베타 버전의) ONNX Exporter입니다.</em> <code>torch.onnx.export</code> 는 PyuTorch 1.2.0부터 지원 중인 TorchScript 백엔드에 기반한 ONNX Exporter입니다.</p>\n",
    "</div>\n",
    "\n",
    "이 튜토리얼에서는 TorchScript 기반의 ONNX Exporter인 `torch.onnx.export`\n",
    "를 사용하여 PyTorch에서 정의한 모델을 어떻게 ONNX 형식으로 변환하는지를\n",
    "살펴보도록 하겠습니다.\n",
    "\n",
    "이렇게 변환된 모델은 ONNX 런타임(Runtime)에서 실행됩니다. ONNX 런타임은\n",
    "다양한 플랫폼과 하드웨어(윈도우즈, 리눅스, 맥 및 CPU, GPU 모두)에서\n",
    "효율적으로 추론하는, 성능에 초점을 맞춘 ONNX 모델을 위한 엔진입니다.\n",
    "\n",
    "[여기](https://cloudblogs.microsoft.com/opensource/2019/05/22/onnx-runtime-machine-learning-inferencing-0-4-release)\n",
    "에서 설명한 것처럼 ONNX 런타임을 활용하면 여러 모델들의 성능을 상당히\n",
    "높일 수 있다는 것이 증명되었습니다.\n",
    "\n",
    "이 튜토리얼의 진행을 위해 [ONNX](https://github.com/onnx/onnx) 및 [ONNX\n",
    "런타임(Runtime)](https://github.com/microsoft/onnxruntime) 의 설치가\n",
    "필요합니다.\n",
    "\n",
    "ONNX 및 ONNX 런타임은 다음과 같이 설치할 수 있습니다:\n",
    "\n",
    "``` {.sourceCode .bash}\n",
    "%%bash\n",
    "pip install onnx onnxruntime\n",
    "```\n",
    "\n",
    "ONNX 런타임은 최신 버전의 PyTorch 런타임을 사용하는 것을 권장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 import문\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초해상화(super-resolution)란 이미지나 비디오의 해상도를 높이기 위한\n",
    "방법으로 이미지 프로세싱이나 비디오 편집에 널리 사용되고 있는\n",
    "방법입니다. 이 튜토리얼에서는 크기가 작은 초해상화 모델을 사용하도록\n",
    "하겠습니다.\n",
    "\n",
    "먼저, 초해상화 모델을 PyTorch에서 구현하겠습니다. 이 모델은 [\\\"Real-Time\n",
    "Single Image and Video Super-Resolution Using an Efficient Sub-Pixel\n",
    "Convolutional Neural Network\\\" - Shi et\n",
    "al](https://arxiv.org/abs/1609.05158) 에서 소개된 효율적인 서브픽셀\n",
    "합성곱 계층을 사용하여 이미지의 해상도를 업스케일 인자만큼 늘립니다.\n",
    "모델은 이미지의 YCbCr 성분 중 Y 성분을 입력값으로 받고 업스케일된\n",
    "초해상도의 Y 채널 값을 리턴합니다.\n",
    "\n",
    "아래는 PyTorch 예제의\n",
    "[모델](https://github.com/pytorch/examples/blob/master/super_resolution/model.py)\n",
    "을 그대로 가져온 것입니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 08:25:40.170349: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 08:25:40.867372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:03<00:00, 54.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import sys\n",
    "import os\n",
    "# 현재 작업 디렉토리 가져오기\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# rednet_plus 디렉토리를 sys.path에 추가 (중복 제거)\n",
    "sys.path.append(os.path.join(os.getcwd(), 'ReDNetPlus'))\n",
    "    \n",
    "from models.rednetplus import ReDNetPlus\n",
    "\n",
    "torch_model = ReDNetPlus(nclass=classes, backbone=backbone).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 과정이 끝나면 일반적인 경우에 모델을 학습시키기 시작할 것입니다.\n",
    "하지만 본 튜토리얼에서는 미리 학습된 가중치들을 사용하도록 하겠습니다.\n",
    "참고로 이 모델은 높은 정확도에 이를 때까지 학습되지 않았고 본 튜토리얼을\n",
    "원활히 진행하기 위한 목적으로 사용하는 것입니다.\n",
    "\n",
    "모델을 변환하기 전에 모델을 추론 모드로 바꾸기 위해서\n",
    "`torch_model.eval()` 또는 `torch_model.train(False)` 를 호출하는 것이\n",
    "중요합니다. 이는 dropout이나 batchnorm과 같은 연산들이 추론과 학습\n",
    "모드에서 다르게 작동하기 때문에 필요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReDNetPlus(\n",
       "  (pretrained): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (head): REDNetHead(\n",
       "    (conv5a): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv5c): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (sa): Attention_Mod(\n",
       "      (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (conv51): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv52): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Dropout2d(p=0.1, inplace=False)\n",
       "      (1): Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (conv7): Sequential(\n",
       "      (0): Dropout2d(p=0.1, inplace=False)\n",
       "      (1): Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (conv8): Sequential(\n",
       "      (0): Dropout2d(p=0.1, inplace=False)\n",
       "      (1): Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (head2): REDNetHead(\n",
       "    (conv5a): Sequential(\n",
       "      (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv5c): Sequential(\n",
       "      (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (sa): Attention_Mod(\n",
       "      (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (conv51): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv52): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Dropout2d(p=0.1, inplace=False)\n",
       "      (1): Conv2d(512, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (conv7): Sequential(\n",
       "      (0): Dropout2d(p=0.1, inplace=False)\n",
       "      (1): Conv2d(512, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (conv8): Sequential(\n",
       "      (0): Dropout2d(p=0.1, inplace=False)\n",
       "      (1): Conv2d(512, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (upsample_11): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "  (conv5c): Sequential(\n",
       "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv5a): Sequential(\n",
       "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (sc): Attention_Mod(\n",
       "    (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (sa): Attention_Mod(\n",
       "    (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (value_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (conv52): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv51): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv7): Sequential(\n",
       "    (0): Dropout2d(p=0.1, inplace=False)\n",
       "    (1): Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (conv6): Sequential(\n",
       "    (0): Dropout2d(p=0.1, inplace=False)\n",
       "    (1): Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (conv8): Sequential(\n",
       "    (0): Dropout2d(p=0.1, inplace=False)\n",
       "    (1): Conv2d(512, 13, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (global_context): GlobalPooling(\n",
       "    (gap): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=1)\n",
       "      (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (munet): MUNet(\n",
       "    (down1): encoder(\n",
       "      (down_conv): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (down2): encoder(\n",
       "      (down_conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (down3): encoder(\n",
       "      (down_conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (middle_conv): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (up1): decoder(\n",
       "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (up_conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (up2): decoder(\n",
       "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (up_conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (up3): decoder(\n",
       "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (up_conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미리 학습된 가중치를 읽어옵니다\n",
    "model_url = 'ReDNetPlus/exp/Harvey_harvey-feb13-2021-2/pspnet101/model/train_epoch_100.pth'\n",
    "batch_size = 2    # 임의의 수\n",
    "\n",
    "# 모델을 미리 학습된 가중치로 초기화합니다\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "# torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# 모델을 추론 모드로 전환합니다\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Tracing이나 스크립팅을 통해서 PyTorch 모델을 변환할 수 있습니다. 이\n",
    "튜토리얼에서는 tracing을 통해 변환된 모델을 사용하도록 하겠습니다.\n",
    "모델을 변환하기 위해서는 `torch.onnx.export()` 함수를 호출합니다. 이\n",
    "함수는 모델을 실행하여 어떤 연산자들이 출력값을 계산하는데\n",
    "사용되었는지를 기록합니다. `export` 함수가 모델을 실행하기 때문에,\n",
    "우리가 직접 텐서를 입력값으로 넘겨주어야 합니다. 이 텐서의 값은 알맞은\n",
    "자료형과 모양이라면 랜덤하게 결정되어도 무방합니다. 특정 차원을 동적\n",
    "차원으로 지정하지 않는 이상, ONNX로 변환된 그래프의 경우 입력값의 크기는\n",
    "모든 차원에 대해 고정됩니다. 예시에서는 모델이 항상 배치 사이즈 1을\n",
    "사용하도록 변환하였지만, `torch.onnx.export()` 의 `dynamic_axes` 인자의\n",
    "첫번째 차원은 동적 차원으로 지정합니다. 따라서 변환된 모델은 임의의\n",
    "batch\\_size에 대해 \\[batch\\_size, 1, 224, 224\\] 사이즈 입력값을 받을 수\n",
    "있습니다.\n",
    "\n",
    "PyTorch의 변환 인터페이스에 대해 더 자세히 알고 싶다면 [torch.onnx\n",
    "문서](https://pytorch.org/docs/master/onnx.html) 를 참고해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 대한 입력값\n",
    "x = torch.randn(batch_size, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3809: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "# 모델 변환\n",
    "torch.onnx.export(torch_model,               # 실행될 모델\n",
    "                  x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)\n",
    "                  \"ReDNetPlus.onnx\",   # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)\n",
    "                  verbose=False) # verbose가 True면 직렬화 과정이 생략, 우리가 볼 수 있는 string값으로 저장되기 때문에 오래 걸린다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX 런타임에서 변환된 모델을 사용했을 때 같은 결과를 얻는지 확인하기\n",
    "위해서 `torch_out` 를 계산합니다.\n",
    "\n",
    "ONNX 런타임에서의 모델 결과값을 확인하기 전에 먼저 ONNX API를 사용해\n",
    "ONNX 모델을 확인해보도록 하겠습니다. 먼저,\n",
    "`onnx.load(\"super_resolution.onnx\")` 는 저장된 모델을 읽어온 후 머신러닝\n",
    "모델을 취합하여 저장하고 있는 상위 파일 컨테이너인 `onnx.ModelProto` 를\n",
    "리턴합니다. `onnx.ModelProto` 에 대해 더 자세한 것은 [onnx.proto\n",
    "기술문서](https://github.com/onnx/onnx/blob/master/onnx/onnx.proto) 에서\n",
    "확인하실 수 있습니다. `onnx.checker.check_model(onnx_model)` 는 모델의\n",
    "구조를 확인하고 모델이 유효한 스키마(valid schema)를 가지고 있는지를\n",
    "체크합니다. ONNX 그래프의 유효성은 모델의 버전, 그래프 구조, 노드들,\n",
    "그리고 입력값과 출력값들을 모두 체크하여 결정됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"ReDNetPlus.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ONNX 런타임의 Python API를 통해 결과값을 계산해보도록 하겠습니다.\n",
    "이 부분은 보통 별도의 프로세스 또는 별도의 머신에서 실행되지만, 이\n",
    "튜토리얼에서는 모델이 ONNX 런타임과 PyTorch에서 동일한 결과를\n",
    "출력하는지를 확인하기 위해 동일한 프로세스에서 계속 실행하도록\n",
    "하겠습니다.\n",
    "\n",
    "모델을 ONNX 런타임에서 실행하기 위해서는 미리 설정된 인자들(본\n",
    "예제에서는 기본값을 사용합니다)로 모델을 위한 추론 세션을 생성해야\n",
    "합니다. 세션이 생성되면, 모델의 run() API를 사용하여 모델을 실행합니다.\n",
    "이 API의 리턴값은 ONNX 런타임에서 연산된 모델의 결과값들을 포함하고 있는\n",
    "리스트입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"ReDNetPlus.onnx\")\n",
    "\n",
    "torch_out = torch_model(x)\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# ONNX 런타임에서 계산된 결과값 \n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# ONNX 런타임과 PyTorch에서 연산된 결과값 비교\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 PyTorch와 ONNX 런타임에서 연산된 결과값이 서로 일치하는지 오차범위(\n",
    "`rtol=1e-03` 와 `atol=1e-05`) 이내에서 확인해야 합니다. 만약 결과가\n",
    "일치하지 않는다면 ONNX 변환기에 문제가 있는 것이니 저희에게 알려주시기\n",
    "바랍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX 런타임에서 이미지를 입력값으로 모델을 실행하기\n",
    "===================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 PyTorch 모델을 변환하고 어떻게 ONNX 런타임에서 구동하는지\n",
    "가상의 텐서를 입력값으로 하여 살펴보았습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 튜토리얼에서는 아래와 같은 유명한 고양이 사진을 사용하도록\n",
    "하겠습니다.\n",
    "\n",
    "![](https://tutorials.pytorch.kr/_static/img/cat_224x224.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, PIL 라이브러리를 사용하여 이미지를 로드하고 전처리하겠습니다. 이\n",
    "전처리는 신경망 학습과 테스트에 보편적으로 적용되고 있는 전처리\n",
    "과정입니다.\n",
    "\n",
    "먼저 이미지를 모델의 입력값 크기(224x224)에 맞게 리사이즈합니다.\n",
    "\n",
    ":   그리고 이미지를 Y, Cb, Cr 성분으로 분해합니다.\n",
    "\n",
    "Y 성분\\[역자 주: 휘도 성분\\]은 그레이스케일(회색조) 이미지를 나타내고,\n",
    "Cb 성분은 파란색에서 밝기를 뺀 색차 성분, Cr은 빨강색에서 밝기를 뺀 색차\n",
    "성분을 나타냅니다. 사람의 눈은 Y 성분에 더 민감하게 반응하기 때문에\n",
    "저희에게는 현재 이 성분이 중요하고, 이 Y 성분을 변환할 것입니다. Y\n",
    "성분을 뽑아낸 뒤에, 추출한 Y 성분을 모델의 입력값이 될 텐서로\n",
    "변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img = Image.open(\"./6419.jpg\")\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "\n",
    "# YCbCr 색 공간으로 변환하고 Y 채널 추출\n",
    "img_ycbcr = img.convert('YCbCr')\n",
    "img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "# 이미지를 텐서로 변환하고 배치 차원 추가\n",
    "to_tensor = transforms.ToTensor()\n",
    "img = to_tensor(img)\n",
    "img = img.unsqueeze(0)  # 배치 차원 추가, (1, 3, 224, 224)\n",
    "img = torch.cat([img, img], dim=0)  # (2, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열로 변환 함수\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 리사이즈된 그레이스케일 고양이 이미지 텐서를 앞서 설명했던 것처럼\n",
    "초해상도 모델에 넘겨주어 ONNX 런타임에서 실행하도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input.1 for the following indices\n index: 0 Got: 1 Expected: 2\n Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ort_inputs \u001b[38;5;241m=\u001b[39m {ort_session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname: to_numpy(img)}\n\u001b[0;32m----> 2\u001b[0m ort_outs \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m img_out_y \u001b[38;5;241m=\u001b[39m ort_outs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input.1 for the following indices\n index: 0 Got: 1 Expected: 2\n Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_y = ort_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 출력과 ONNX Runtime 출력이 허용 오차 내에서 거의 동일합니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ReDNetPlus 모델 임포트\n",
    "from models.rednetplus import ReDNetPlus\n",
    "\n",
    "# 이미지 로드\n",
    "img = Image.open(\"./6419.jpg\")\n",
    "\n",
    "# 이미지 크기 조정\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "\n",
    "# 이미지를 텐서로 변환하고 배치 차원 추가\n",
    "to_tensor = transforms.ToTensor()\n",
    "img = to_tensor(img)\n",
    "img = img.unsqueeze(0)  # 배치 차원 추가, (1, 3, 224, 224)\n",
    "img = torch.cat([img, img], dim=0)  # (2, 3, 224, 224)\n",
    "\n",
    "# NumPy 배열로 변환 함수\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model_path = \"ReDNetPlus.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# ONNX Runtime에서 모델 실행\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_y = ort_outs[0]\n",
    "\n",
    "# PyTorch 모델의 출력\n",
    "torch_model = ReDNetPlus(nclass=10, backbone='resnet101').cuda()\n",
    "img = img.cuda()\n",
    "\n",
    "torch_out = torch_model(img)\n",
    "\n",
    "# 비교 (허용 오차 조정)\n",
    "# np.testing.assert_allclose(to_numpy(torch_out), img_out_y, rtol=1e-02, atol=1e-04)\n",
    "\n",
    "print(\"PyTorch 출력과 ONNX Runtime 출력이 허용 오차 내에서 거의 동일합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- --------------\n",
      "absl-py                      2.1.0\n",
      "alembic                      1.13.1\n",
      "anyio                        4.4.0\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "attrs                        23.2.0\n",
      "Babel                        2.15.0\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.12.3\n",
      "bleach                       6.1.0\n",
      "cachetools                   5.3.3\n",
      "catboost                     1.2.5\n",
      "category-encoders            2.6.3\n",
      "certifi                      2024.2.2\n",
      "cffi                         1.16.0\n",
      "charset-normalizer           3.3.2\n",
      "cloudpickle                  3.0.0\n",
      "coloredlogs                  15.0.1\n",
      "colorlog                     6.8.2\n",
      "colour                       0.1.5\n",
      "comm                         0.2.2\n",
      "contourpy                    1.1.1\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.1\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "dtreeviz                     2.2.2\n",
      "exceptiongroup               1.2.1\n",
      "executing                    2.0.1\n",
      "fastjsonschema               2.19.1\n",
      "filelock                     3.14.0\n",
      "flatbuffers                  24.3.25\n",
      "fonttools                    4.52.4\n",
      "fqdn                         1.5.1\n",
      "fsspec                       2024.5.0\n",
      "gast                         0.4.0\n",
      "google-auth                  2.29.0\n",
      "google-auth-oauthlib         1.0.0\n",
      "google-pasta                 0.2.0\n",
      "graphviz                     0.20.3\n",
      "greenlet                     3.0.3\n",
      "grpcio                       1.64.0\n",
      "h11                          0.14.0\n",
      "h5py                         3.11.0\n",
      "httpcore                     1.0.5\n",
      "httpx                        0.27.0\n",
      "humanfriendly                10.0\n",
      "idna                         3.7\n",
      "importlib_metadata           7.1.0\n",
      "importlib_resources          6.4.0\n",
      "iniconfig                    2.0.0\n",
      "ipykernel                    6.29.4\n",
      "ipython                      8.12.3\n",
      "ipywidgets                   8.1.3\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "Jinja2                       3.1.4\n",
      "joblib                       1.4.2\n",
      "json5                        0.9.25\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.22.0\n",
      "jsonschema-specifications    2023.12.1\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.6.2\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.7.2\n",
      "jupyter-events               0.10.0\n",
      "jupyter-lsp                  2.2.5\n",
      "jupyter_server               2.14.0\n",
      "jupyter_server_terminals     0.5.3\n",
      "jupyterlab                   4.2.1\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.27.2\n",
      "jupyterlab_widgets           3.0.11\n",
      "keras                        2.13.1\n",
      "kiwisolver                   1.4.5\n",
      "libclang                     18.1.1\n",
      "lightgbm                     4.3.0\n",
      "llvmlite                     0.41.1\n",
      "Mako                         1.3.5\n",
      "Markdown                     3.6\n",
      "MarkupSafe                   2.1.5\n",
      "matplotlib                   3.7.5\n",
      "matplotlib-inline            0.1.7\n",
      "mistune                      3.0.2\n",
      "mljar-scikit-plot            0.3.10\n",
      "mljar-supervised             1.1.8\n",
      "mpmath                       1.3.0\n",
      "nbclient                     0.10.0\n",
      "nbconvert                    7.16.4\n",
      "nbformat                     5.10.4\n",
      "nest-asyncio                 1.6.0\n",
      "networkx                     3.1\n",
      "notebook                     7.2.0\n",
      "notebook_shim                0.2.4\n",
      "numba                        0.58.1\n",
      "numpy                        1.24.4\n",
      "nvidia-cublas-cu12           12.1.3.1\n",
      "nvidia-cuda-cupti-cu12       12.1.105\n",
      "nvidia-cuda-nvrtc-cu12       12.1.105\n",
      "nvidia-cuda-runtime-cu12     12.1.105\n",
      "nvidia-cudnn-cu12            8.9.2.26\n",
      "nvidia-cufft-cu12            11.0.2.54\n",
      "nvidia-curand-cu12           10.3.2.106\n",
      "nvidia-cusolver-cu12         11.4.5.107\n",
      "nvidia-cusparse-cu12         12.1.0.106\n",
      "nvidia-nccl-cu12             2.20.5\n",
      "nvidia-nvjitlink-cu12        12.5.40\n",
      "nvidia-nvtx-cu12             12.1.105\n",
      "oauthlib                     3.2.2\n",
      "onnx                         1.16.1\n",
      "onnxruntime                  1.18.1\n",
      "opencv-python                4.9.0.80\n",
      "opt-einsum                   3.3.0\n",
      "optuna                       3.6.1\n",
      "overrides                    7.7.0\n",
      "packaging                    24.0\n",
      "pandas                       2.0.3\n",
      "pandocfilters                1.5.1\n",
      "parso                        0.8.4\n",
      "patsy                        0.5.6\n",
      "pexpect                      4.9.0\n",
      "pickleshare                  0.7.5\n",
      "pillow                       10.3.0\n",
      "pip                          24.0\n",
      "pkgutil_resolve_name         1.3.10\n",
      "platformdirs                 4.2.2\n",
      "plotly                       5.22.0\n",
      "pluggy                       1.5.0\n",
      "prometheus_client            0.20.0\n",
      "prompt_toolkit               3.0.45\n",
      "protobuf                     4.25.3\n",
      "psutil                       5.9.8\n",
      "ptyprocess                   0.7.0\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.6.0\n",
      "pyasn1_modules               0.4.0\n",
      "pycparser                    2.22\n",
      "Pygments                     2.18.0\n",
      "pyparsing                    3.1.2\n",
      "pytest                       8.2.1\n",
      "python-dateutil              2.9.0.post0\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2024.1\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        26.0.3\n",
      "qtconsole                    5.5.2\n",
      "QtPy                         2.4.1\n",
      "referencing                  0.35.1\n",
      "requests                     2.32.2\n",
      "requests-oauthlib            2.0.0\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rpds-py                      0.18.1\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.2\n",
      "scipy                        1.10.1\n",
      "seaborn                      0.13.2\n",
      "Send2Trash                   1.8.3\n",
      "setuptools                   45.2.0\n",
      "shap                         0.44.1\n",
      "six                          1.16.0\n",
      "slicer                       0.0.7\n",
      "sniffio                      1.3.1\n",
      "soupsieve                    2.5\n",
      "SQLAlchemy                   2.0.30\n",
      "stack-data                   0.6.3\n",
      "statsmodels                  0.14.1\n",
      "sympy                        1.12\n",
      "tabulate                     0.9.0\n",
      "tenacity                     8.3.0\n",
      "tensorboard                  2.13.0\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorboardX                 2.6.2.2\n",
      "tensorflow                   2.13.1\n",
      "tensorflow-estimator         2.13.0\n",
      "tensorflow-io-gcs-filesystem 0.34.0\n",
      "termcolor                    2.4.0\n",
      "terminado                    0.18.1\n",
      "threadpoolctl                3.5.0\n",
      "tinycss2                     1.3.0\n",
      "tomli                        2.0.1\n",
      "torch                        2.3.0\n",
      "torchvision                  0.18.0\n",
      "tornado                      6.4\n",
      "tqdm                         4.66.4\n",
      "traitlets                    5.14.3\n",
      "transforms                   0.2.1\n",
      "triton                       2.3.0\n",
      "types-python-dateutil        2.9.0.20240316\n",
      "typing_extensions            4.12.2\n",
      "tzdata                       2024.1\n",
      "uri-template                 1.3.0\n",
      "urllib3                      2.2.1\n",
      "wcwidth                      0.2.13\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.8.0\n",
      "Werkzeug                     3.0.3\n",
      "wheel                        0.34.2\n",
      "widgetsnbextension           4.0.11\n",
      "wordcloud                    1.9.3\n",
      "wrapt                        1.16.0\n",
      "xgboost                      2.0.3\n",
      "zipp                         3.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sgmllib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mext_transforms\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(ext_transforms\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(transforms))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transforms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_html\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_html, bodyfinder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# modules = [\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     'st',             # zopish\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     'rest',           # docutils\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     'html_to_web_intelligent_plain_text',\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transforms/safe_html.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msgmllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGMLParser, SGMLParseError\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sgmllib'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import transforms as ext_transforms\n",
    "\n",
    "print(ext_transforms.__version__)\n",
    "print(dir(transforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 하면 모델의 결과값은 텐서가 됩니다. 이제 모델의 결과값을 처리하여\n",
    "결과값 텐서에서 마지막 최종 출력 이미지를 만들고 이를 저장해보도록\n",
    "하겠습니다. 후처리 단계는\n",
    "[링크](https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py)\n",
    "에 구현되어있는 초해상도 모델 코드에서 가져온 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sgmllib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mext_transforms\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Harvey \u001b[38;5;28;01mas\u001b[39;00m dataset\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transforms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_html\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_html, bodyfinder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# modules = [\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     'st',             # zopish\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     'rest',           # docutils\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     'html_to_web_intelligent_plain_text',\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transforms/safe_html.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msgmllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGMLParser, SGMLParseError\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sgmllib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import transforms as ext_transforms\n",
    "from PIL import Image\n",
    "from data import Harvey as dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "\n",
    "# 기존 설정\n",
    "data_root = './ReDNetPlus/dataset/Harvey_harvey-feb13-2021-2'\n",
    "train_h = 713\n",
    "train_w = 713\n",
    "workers = 4\n",
    "model_path = './ReDNetPlus/exp/Harvey_harvey-feb13-2021-2/pspnet101/model/train_epoch_100.pth'\n",
    "colors_path = './ReDNetPlus/dataset/michael/list/michael_colors.txt'\n",
    "names_path = './ReDNetPlus/dataset/michael/list/michael_names.txt'\n",
    "output_dir = 'outputs'\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# 평균 및 표준편차 설정\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# 이미지와 라벨 변환\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((train_h, train_w)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize((train_h, train_w), Image.NEAREST),\n",
    "    ext_transforms.PILToLongTensor()\n",
    "])\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_data = dataset(\n",
    "    data_root,\n",
    "    transform=image_transform,\n",
    "    label_transform=label_transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def net_process(model, image, mean, std=None, flip=True):\n",
    "    input = torch.from_numpy(image.transpose((2, 0, 1))).float()\n",
    "    if std is None:\n",
    "        for t, m in zip(input, mean):\n",
    "            t.sub_(m)\n",
    "    else:\n",
    "        for t, m, s in zip(input, mean, std):\n",
    "            t.sub_(m).div_(s)\n",
    "    input = input.unsqueeze(0).cuda()\n",
    "    if flip:\n",
    "        input = torch.cat([input, input.flip(3)], 0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "    _, _, h_i, w_i = input.shape\n",
    "    _, _, h_o, w_o = output.shape\n",
    "    if (h_o != h_i) or (w_o != w_i):\n",
    "        output = F.interpolate(output, (h_i, w_i), mode='bilinear', align_corners=True)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    if flip:\n",
    "        output = (output[0] + output[1].flip(2)) / 2\n",
    "    else:\n",
    "        output = output[0]\n",
    "    output = output.data.cpu().numpy()\n",
    "    output = output.transpose(1, 2, 0)\n",
    "    return output\n",
    "\n",
    "def predict(model, images, paths, class_encoding, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    _, predictions = torch.max(predictions.data, 1)\n",
    "    label_to_rgb = transforms.Compose([\n",
    "        ext_transforms.LongTensorToRGBPIL(class_encoding),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    color_predictions = utils.batch_transform(predictions.cpu(), label_to_rgb)\n",
    "    \n",
    "    for predict, impath in zip(color_predictions, paths):\n",
    "        outname = os.path.splitext(impath)[0] + '.png'\n",
    "        outname_final = os.path.join(output_dir, outname)\n",
    "        save_image(predict, outname_final)\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 모델 로드\n",
    "    model = torch.load(model_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 클래스 인코딩 로드\n",
    "    class_encoding = test_data.color_encoding\n",
    "\n",
    "    # 테스트 데이터를 사용하여 예측 수행\n",
    "    for batch in test_loader:\n",
    "        images, paths = batch\n",
    "        predict(model, images, paths, class_encoding, output_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n",
    "\n",
    "# PyTorch 버전의 후처리 과정 코드를 이용해 결과 이미지 만들기\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")\n",
    "\n",
    "# 이미지를 저장하고 모바일 기기에서의 결과 이미지와 비교하기\n",
    "final_img.save(\"./cat_superres_with_ort.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tutorials.pytorch.kr/_static/img/cat_superres_with_ort.jpg)\n",
    "\n",
    "ONNX 런타임은 크로스플랫폼 엔진으로서 CPU와 GPU 뿐만 아니라 여러\n",
    "플랫폼에서 실행될 수 있습니다.\n",
    "\n",
    "ONNX 런타임은 Azure Machine Learning Services와 같은 클라우드에 배포되어\n",
    "모델 추론을 하는데 사용될 수도 있습니다. 더 자세한 내용은\n",
    "[링크](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-onnx)\n",
    "를 참조해주십시오.\n",
    "\n",
    "ONNX 런타임의 성능에 관한 것은\n",
    "[여기](https://github.com/microsoft/onnxruntime#high-performance) 에서\n",
    "확인하실 수 있습니다.\n",
    "\n",
    "ONNX 런타임에 관한 더 자세한 내용은 [이\n",
    "곳](https://github.com/microsoft/onnxruntime) 을 참조해 주세요.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
